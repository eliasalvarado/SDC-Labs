{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba14c8c",
   "metadata": {},
   "source": [
    "# Laboratorio #9 - Ataque y defensa de modelos de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90877b",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ealva\\Documents\\GitHub\\SDC-Labs\\envLabs8_9\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ealva\\Documents\\GitHub\\SDC-Labs\\envLabs8_9\\lib\\site-packages\\art\\estimators\\certification\\__init__.py:29: UserWarning: PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\n",
      "  warnings.warn(\"PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.extraction import CopycatCNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from tensorflow.keras.models import clone_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from art.defences.postprocessor import ReverseSigmoid\n",
    "from art.attacks.inference.membership_inference import MembershipInferenceBlackBoxRuleBased\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82d8a3",
   "metadata": {},
   "source": [
    "# Primer ataque - Ataque de Extracción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85593b77",
   "metadata": {},
   "source": [
    "Como primer ataque escogí un ataque de extracción ya que me parece que es uno de los ataques que cualquier modelo no se puede salvar, debido a que siempre habrá alguien que desee apropiarse del esfuerzo de otro, invirtiendo menos tiempo y dinero.\n",
    "Lo que se busca es replicar la clasificación del modelo víctima utilizando solamente entradas y salidas de un dataset propio. \n",
    "Para lograr esto se entrenará un nuevo modelo solamente utilizando las salidas del modelo víctima, buscando imitar el comportamiento del modelo sin caer en la necesidad de un entrenamiento robusto pero buscando tener la información suficiente como para hacer que el nuevo modelo cuente con los pesos aproximados del modelo víctima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a700f3",
   "metadata": {},
   "source": [
    "## Ataque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e6ae2",
   "metadata": {},
   "source": [
    "### Cargar el modelo víctima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87b5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_model = load_model('../Laboratorio8/malimg_model_saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c5b4b",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3eee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6094 images belonging to 22 classes.\n",
      "Found 1513 images belonging to 22 classes.\n",
      "Datos: X:(2560, 457, 340, 1) y:(2560, 22)\n"
     ]
    }
   ],
   "source": [
    "datasetPath = \"../Laboratorio8/malimg_paper_dataset_imgs\"\n",
    "avgHigh, avgWidth = 457, 340\n",
    "batch_size = 64\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "attack_generator = datagen.flow_from_directory(\n",
    "    datasetPath,\n",
    "    target_size=(avgHigh, avgWidth),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    datasetPath,\n",
    "    target_size=(avgHigh, avgWidth),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for _ in range(40):  # 40 batches de 64 = 2560 muestras\n",
    "    Xb, yb = next(attack_generator)\n",
    "    X_list.append(Xb)\n",
    "    y_list.append(yb)\n",
    "X_attack = np.concatenate(X_list)\n",
    "y_attack = np.concatenate(y_list)\n",
    "X_validation, y_validation = next(validation_generator)\n",
    "print(f\"Datos: X:{X_attack.shape} y:{y_attack.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad94b4",
   "metadata": {},
   "source": [
    "### Clasificador atacante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6232103",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "nb_classes = vulnerable_model.output_shape[-1]\n",
    "input_shape = vulnerable_model.input_shape[1:]\n",
    "\n",
    "clasificador_victima = TensorFlowV2Classifier(\n",
    "    model=vulnerable_model,\n",
    "    loss_object=loss_object,\n",
    "    nb_classes=nb_classes,\n",
    "    input_shape=input_shape,\n",
    "    clip_values=(0, 1)\n",
    ")\n",
    "\n",
    "attacker_model = clone_model(vulnerable_model)\n",
    "attacker_model.build(input_shape=(None, *input_shape))\n",
    "attacker_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=loss_object,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "clasificador_atacante = TensorFlowV2Classifier(\n",
    "    model=attacker_model,\n",
    "    loss_object=loss_object,\n",
    "    nb_classes=nb_classes,\n",
    "    input_shape=input_shape,\n",
    "    clip_values=(0, 1),\n",
    "    optimizer=attacker_model.optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd19f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = CopycatCNN(\n",
    "    classifier=clasificador_victima,\n",
    "    batch_size_fit=32,\n",
    "    nb_epochs=20,\n",
    "    nb_stolen=len(X_attack)\n",
    ")\n",
    "\n",
    "stolen_classifier = attack.extract(X_attack, y_attack, thieved_classifier=clasificador_atacante)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10d473",
   "metadata": {},
   "source": [
    "### Evaluación de resultados - Ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9204ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo robado: 0.7812\n"
     ]
    }
   ],
   "source": [
    "preds = stolen_classifier.predict(X_validation)\n",
    "accuracy = np.mean(np.argmax(preds, axis=1) == np.argmax(y_validation, axis=1))\n",
    "print(f\"Precisión del modelo robado: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afca644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo víctima: 0.7812\n",
      "Precisión del modelo robado: 0.7812\n"
     ]
    }
   ],
   "source": [
    "preds_victima = clasificador_victima.predict(X_validation)\n",
    "acc_victima = np.mean(np.argmax(preds_victima, axis=1) == np.argmax(y_validation, axis=1))\n",
    "print(f\"Precisión del modelo víctima: {acc_victima:.4f}\")\n",
    "\n",
    "preds_robado = stolen_classifier.predict(X_validation)\n",
    "acc_robado = np.mean(np.argmax(preds_robado, axis=1) == np.argmax(y_validation, axis=1))\n",
    "print(f\"Precisión del modelo robado: {acc_robado:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa33454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 1: Real=1, Víctima=1, Robado=1\n",
      "Ejemplo 2: Real=5, Víctima=21, Robado=21\n",
      "Ejemplo 3: Real=9, Víctima=9, Robado=9\n",
      "Ejemplo 4: Real=10, Víctima=10, Robado=10\n",
      "Ejemplo 5: Real=15, Víctima=15, Robado=15\n",
      "Ejemplo 6: Real=21, Víctima=21, Robado=21\n",
      "Ejemplo 7: Real=7, Víctima=7, Robado=7\n",
      "Ejemplo 8: Real=20, Víctima=20, Robado=20\n",
      "Ejemplo 9: Real=20, Víctima=20, Robado=20\n",
      "Ejemplo 10: Real=8, Víctima=9, Robado=9\n",
      "\n",
      "Coincidencias entre víctima y robado: 32/32 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "labels_victima = np.argmax(preds_victima, axis=1)\n",
    "labels_robado = np.argmax(preds_robado, axis=1)\n",
    "labels_true = np.argmax(y_validation, axis=1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Ejemplo {i + 1}: Real={labels_true[i]}, Víctima={labels_victima[i]}, Robado={labels_robado[i]}\")\n",
    "\n",
    "coinciden = np.sum(labels_victima == labels_robado)\n",
    "print(f\"\\nCoincidencias entre víctima y robado: {coinciden}/{len(labels_true)} ({coinciden/len(labels_true):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "464c64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.6322 - accuracy: 0.7812\n",
      "1/1 [==============================] - 1s 1s/step - loss: 108.0209 - accuracy: 0.7812\n",
      "Original test loss: 0.63 vs stolen test loss: 108.02\n",
      "Original test accuracy: 0.78 vs stolen test accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Testing the performance of the original classifier\n",
    "score_original = vulnerable_model.evaluate(\n",
    "    x=X_validation,\n",
    "    y=y_validation\n",
    "    )\n",
    "\n",
    "# Testing the performance of the stolen classifier\n",
    "score_stolen = stolen_classifier.model.evaluate(\n",
    "    x=X_validation, \n",
    "    y=y_validation\n",
    "    )\n",
    "\n",
    "# Comparing test losses\n",
    "print(f\"Original test loss: {score_original[0]:.2f} \" \n",
    "      f\"vs stolen test loss: {score_stolen[0]:.2f}\")\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Original test accuracy: {score_original[1]:.2f} \" \n",
    "      f\"vs stolen test accuracy: {score_stolen[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f7aed",
   "metadata": {},
   "source": [
    "Como se pudo observar, el ataque logró replicar muy bien el comportamiento del modelo original (vulnerable_model). Haciendo que el modelo creado a partir de él, tuviera resultados parecidos. Aunque en cuanto a la predicción de los modelos se puede ver la misma, la pérdida es muy distinta. Mientras que la pérdida del modelo original es de tan solo 0.63, la del modelo robado es de 108.02, esto se puedo haber generado por cómo se entrenó este modelo robado. Haciendo que las distribuciones de probabilidad sean más dispersas que las del modelo original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae7b1f",
   "metadata": {},
   "source": [
    "## Defensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875fb6c1",
   "metadata": {},
   "source": [
    "### Implementación de la defensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b61c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_sigmoid = ReverseSigmoid(beta=3.5, apply_fit=False, apply_predict=True)\n",
    "clasificador_victima.postprocessing_defences = [reverse_sigmoid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f5f04",
   "metadata": {},
   "source": [
    "### Clasificador atacante (ya con modelo protegido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b295c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_model_protected = tf.keras.models.clone_model(vulnerable_model)\n",
    "attacker_model_protected.build(input_shape=(None, *input_shape))\n",
    "attacker_model_protected.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=loss_object,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "clasificador_atacante_protegido = TensorFlowV2Classifier(\n",
    "    model=attacker_model_protected,\n",
    "    loss_object=loss_object,\n",
    "    nb_classes=nb_classes,\n",
    "    input_shape=input_shape,\n",
    "    clip_values=(0, 1),\n",
    "    optimizer=attacker_model_protected.optimizer,\n",
    ")\n",
    "\n",
    "attack_protected = CopycatCNN(\n",
    "    batch_size_fit=32,\n",
    "    nb_epochs=20,\n",
    "    nb_stolen=len(X_attack),\n",
    "    classifier=clasificador_victima\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcad8df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ealva\\Documents\\GitHub\\SDC-Labs\\envLabs8_9\\lib\\site-packages\\art\\defences\\postprocessor\\reverse_sigmoid.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  perturbation_r = self.beta * (sigmoid(-self.gamma * np.log((1.0 - preds_clipped) / preds_clipped)) - 0.5)\n"
     ]
    }
   ],
   "source": [
    "stolen_classifier_protected = attack_protected.extract(X_attack, y_attack, thieved_classifier=clasificador_atacante_protegido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dabab272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 979ms/step - loss: 0.6322 - accuracy: 0.7812\n",
      "1/1 [==============================] - 1s 997ms/step - loss: 3859.0566 - accuracy: 0.0000e+00\n",
      "Original test loss: 0.63 vs stolen test loss: 3859.06\n",
      "Original test accuracy: 0.78 vs stolen test accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Testing the performance of the original classifier\n",
    "score_original = clasificador_victima.model.evaluate(\n",
    "    x=X_validation,\n",
    "    y=y_validation\n",
    "    )\n",
    "\n",
    "# Testing the performance of the stolen classifier\n",
    "score_stolen = stolen_classifier_protected.model.evaluate(\n",
    "    x=X_validation, \n",
    "    y=y_validation\n",
    "    )\n",
    "\n",
    "# Comparing test losses\n",
    "print(f\"Original test loss: {score_original[0]:.2f} \" \n",
    "      f\"vs stolen test loss: {score_stolen[0]:.2f}\")\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Original test accuracy: {score_original[1]:.2f} \" \n",
    "      f\"vs stolen test accuracy: {score_stolen[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e519ff",
   "metadata": {},
   "source": [
    "Listo, ahora se puede apreciar como el modelo robado no logró captar el comportamiento del modelo original. Haciendo que su precisión se redujera a 0, lo cuál parece excelente ya que nos asegura que un ataque de extracción haría que el modelo resultante tuviera una precisión muy baja. Esto fue gracias a que Reverse Sigmoid distorciona las probabilidades de salida del modelo y debido a que el ataque sí o sí necesita dichos valores, su alteración hizo que los resultados del ataque se vieran muy afectados. Pues lo que busca es imitar al modelo víctima, y claro, sin la información real es imposible que algo (en cualquier contexto) pueda llegar a imitar el comportamiento. Por eso el modelo resultante del ataque imitó salidas erróneas, provocando inestabilidad en sus resultados y dando resultados nada precisos.\n",
    "\n",
    "Además, como bien se había apreciado antes, aun sin la defensa, el modelo resultante del ataque ya contaba con una pérdida alta a comparación del original. Seguramente eso influyó en no tener la necesidad de implementar una defensa agresiva para hacer que el modelo atacante pudiera converger como lo hizo el modelo original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9aa77",
   "metadata": {},
   "source": [
    "# Segundo ataque - Ataque de Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdcb2e",
   "metadata": {},
   "source": [
    "## Ataque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b02d95",
   "metadata": {},
   "source": [
    "Como segundo ataque seleccioné lo que es un ataque de inferencia. Donde buscaré determianr si una muestra específica fue utilizada como parte del dataset para el entrenamiento del modelo. Entonces como resultado espero tener un modelo el cuál sea capaz de predecir si una imagen fue o no parte del set de entrenamiento del modelo original, explotando la vulnerabilidad que tienen los modelos al mostrar una mayor \"confianza\" en el retorno de su clasificación con una muestra la cuál vió durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d84ac",
   "metadata": {},
   "source": [
    "### Cargar el modelo víctima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583a6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_model_At2 = load_model('../Laboratorio8/malimg_model_saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07643b4",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edb0c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6094 images belonging to 22 classes.\n",
      "Found 1513 images belonging to 22 classes.\n",
      "Datos: X:(2560, 457, 340, 1) y:(2560, 22)\n"
     ]
    }
   ],
   "source": [
    "datasetPath = \"../Laboratorio8/malimg_paper_dataset_imgs\"\n",
    "avgHigh, avgWidth = 457, 340\n",
    "batch_size = 64\n",
    "\n",
    "datagenAt2 = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "attack_generator_At2 = datagenAt2.flow_from_directory(\n",
    "    datasetPath,\n",
    "    target_size=(avgHigh, avgWidth),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator_At2 = datagenAt2.flow_from_directory(\n",
    "    datasetPath,\n",
    "    target_size=(avgHigh, avgWidth),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "X_list_At2, y_list_At2 = [], []\n",
    "for _ in range(40):  # 40 batches de 64 = 2560 muestras\n",
    "    Xb, yb = next(attack_generator_At2)\n",
    "    X_list_At2.append(Xb)\n",
    "    y_list_At2.append(yb)\n",
    "X_attack_At2 = np.concatenate(X_list_At2)\n",
    "y_attack_At2 = np.concatenate(y_list_At2)\n",
    "X_validation_At2, y_validation_At2 = next(validation_generator_At2)\n",
    "print(f\"Datos: X:{X_attack_At2.shape} y:{y_attack_At2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec026dc",
   "metadata": {},
   "source": [
    "### Clasificador víctima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949cf297",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object_At2 = tf.keras.losses.CategoricalCrossentropy()\n",
    "nb_classes_At2 = vulnerable_model_At2.output_shape[-1]\n",
    "input_shape_At2 = vulnerable_model_At2.input_shape[1:]\n",
    "\n",
    "clasificador_victima_At2 = TensorFlowV2Classifier(\n",
    "    model=vulnerable_model_At2,\n",
    "    loss_object=loss_object_At2,\n",
    "    nb_classes=nb_classes_At2,\n",
    "    input_shape=input_shape_At2,\n",
    "    clip_values=(0, 1),\n",
    "    optimizer=vulnerable_model_At2.optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_inferencia = MembershipInferenceBlackBoxRuleBased(classifier=clasificador_victima_At2)\n",
    "\n",
    "X_mi = np.concatenate([X_attack_At2, X_validation_At2])\n",
    "y_mi = np.concatenate([y_attack_At2, y_validation_At2])\n",
    "\n",
    "y_mi_labels = np.concatenate([np.ones(len(X_attack_At2)), np.zeros(len(X_validation_At2))])\n",
    "\n",
    "mi_pred = attack_inferencia.infer(X_mi, y_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e57b8",
   "metadata": {},
   "source": [
    "### Evaluación de resultados - Ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a090c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del ataque de inferencia de membresía: 0.7735\n"
     ]
    }
   ],
   "source": [
    "acc_mi = accuracy_score(y_mi_labels, mi_pred)\n",
    "print(f\"Precisión del ataque de inferencia de membresía: {acc_mi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb0642",
   "metadata": {},
   "source": [
    "## Defensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5ebd8",
   "metadata": {},
   "source": [
    "### Implementación de la defensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc34ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ealva\\Documents\\GitHub\\SDC-Labs\\envLabs8_9\\lib\\site-packages\\art\\defences\\postprocessor\\reverse_sigmoid.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  perturbation_r = self.beta * (sigmoid(-self.gamma * np.log((1.0 - preds_clipped) / preds_clipped)) - 0.5)\n"
     ]
    }
   ],
   "source": [
    "reverse_sigmoid_At2 = ReverseSigmoid(beta=3.5, apply_fit=False, apply_predict=True)\n",
    "clasificador_victima_At2.postprocessing_defences = [reverse_sigmoid_At2]\n",
    "\n",
    "attack_inferencia_def = MembershipInferenceBlackBoxRuleBased(classifier=clasificador_victima_At2)\n",
    "\n",
    "X_mi_defended = np.concatenate([X_attack_At2, X_validation_At2])\n",
    "y_mi_defended = np.concatenate([y_attack_At2, y_validation_At2])\n",
    "y_mi_labels_defended = np.concatenate([np.ones(len(X_attack_At2)), np.zeros(len(X_validation_At2))])\n",
    "\n",
    "mi_pred_def = attack_inferencia_def.infer(X_mi_defended, y_mi_defended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4186564",
   "metadata": {},
   "source": [
    "### Clasificador atacante (ya con modelo protegido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b8990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del ataque de inferencia de membresía (con defensa): 0.0123\n"
     ]
    }
   ],
   "source": [
    "acc_mi_def = accuracy_score(y_mi_labels_defended, mi_pred_def)\n",
    "print(f\"Precisión del ataque de inferencia de membresía (con defensa): {acc_mi_def:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e9d1d",
   "metadata": {},
   "source": [
    "Como bien se puede observar, este ataque inicialmente fue todo un éxito. El ataque mostró una precisión del 77.35%, por lo que se puede saber con gran certeza si una muestra fue o no parte del set de entrenamiento del modelo. Haciendo que el modelo sea vulnerable en este aspecto. Ya que demuestra que el modelo retorna entre su salida la confianza del modelo en sí en esa predicción, haciendo que se pueda llegar a inferir si la muestra fue o no parte del entrenamiento.\n",
    "Luego, al aplicar una defensa utilizando Reverse Sigmoid, la precisión se vino en picada y cayó hasta tener solamente un 1.23% de precisión. Evidentemente la defensa cumplió su rol y ayudó que un ataque de inferencia no fuera posible con el modelo. Esto gracias a que Reverse Sigmoid distorsiona las probabilidades softmax de salida del modelo, provocando que cuando el ataque vea dichas probabilidades llegue a confundir la clasificación gracias a que la probabilidad vista por el ataque no es la real. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLabs8_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
