{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba14c8c",
   "metadata": {},
   "source": [
    "# Laboratorio #9 - Ataque y defensa de modelos de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90877b",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c7e631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.extraction import CopycatCNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from tensorflow.keras.models import clone_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from art.defences.postprocessor import ReverseSigmoid\n",
    "from art.attacks.inference.membership_inference import MembershipInferenceBlackBoxRuleBased\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.attacks.evasion import FastGradientMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82d8a3",
   "metadata": {},
   "source": [
    "# Primer ataque - Ataque de Extracción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85593b77",
   "metadata": {},
   "source": [
    "Como primer ataque escogí un ataque de extracción ya que me parece que es uno de los ataques que cualquier modelo no se puede salvar, debido a que siempre habrá alguien que desee apropiarse del esfuerzo de otro, invirtiendo menos tiempo y dinero.\n",
    "Lo que se busca es replicar la clasificación del modelo víctima utilizando solamente entradas y salidas de un dataset propio. \n",
    "Para lograr esto se entrenará un nuevo modelo solamente utilizando las salidas del modelo víctima, buscando imitar el comportamiento del modelo sin caer en la necesidad de un entrenamiento robusto pero buscando tener la información suficiente como para hacer que el nuevo modelo cuente con los pesos aproximados del modelo víctima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a700f3",
   "metadata": {},
   "source": [
    "## Ataque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e6ae2",
   "metadata": {},
   "source": [
    "### Cargar el modelo víctima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87b5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_model = load_model('../Laboratorio8/malimg_model_saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c5b4b",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3eee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6094 images belonging to 22 classes.\n",
      "Found 1513 images belonging to 22 classes.\n",
      "Datos: X:(2560, 457, 340, 1) y:(2560, 22)\n"
     ]
    }
   ],
   "source": [
    "datasetPath = \"../Laboratorio8/malimg_paper_dataset_imgs\"\n",
    "avgHigh, avgWidth = 457, 340\n",
    "batch_size = 64\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "attack_generator = datagen.flow_from_directory(\n",
    "    datasetPath,\n",
    "    target_size=(avgHigh, avgWidth),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    datasetPath,\n",
    "    target_size=(avgHigh, avgWidth),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for _ in range(40):  # 40 batches de 64 = 2560 muestras\n",
    "    Xb, yb = next(attack_generator)\n",
    "    X_list.append(Xb)\n",
    "    y_list.append(yb)\n",
    "X_attack = np.concatenate(X_list)\n",
    "y_attack = np.concatenate(y_list)\n",
    "X_validation, y_validation = next(validation_generator)\n",
    "print(f\"Datos: X:{X_attack.shape} y:{y_attack.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad94b4",
   "metadata": {},
   "source": [
    "### Clasificador atacante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6232103",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "nb_classes = vulnerable_model.output_shape[-1]\n",
    "input_shape = vulnerable_model.input_shape[1:]\n",
    "\n",
    "clasificador_victima = TensorFlowV2Classifier(\n",
    "    model=vulnerable_model,\n",
    "    loss_object=loss_object,\n",
    "    nb_classes=nb_classes,\n",
    "    input_shape=input_shape,\n",
    "    clip_values=(0, 1)\n",
    ")\n",
    "\n",
    "attacker_model = clone_model(vulnerable_model)\n",
    "attacker_model.build(input_shape=(None, *input_shape))\n",
    "attacker_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=loss_object,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "clasificador_atacante = TensorFlowV2Classifier(\n",
    "    model=attacker_model,\n",
    "    loss_object=loss_object,\n",
    "    nb_classes=nb_classes,\n",
    "    input_shape=input_shape,\n",
    "    clip_values=(0, 1),\n",
    "    optimizer=attacker_model.optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd19f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = CopycatCNN(\n",
    "    classifier=clasificador_victima,\n",
    "    batch_size_fit=32,\n",
    "    nb_epochs=20,\n",
    "    nb_stolen=len(X_attack)\n",
    ")\n",
    "\n",
    "stolen_classifier = attack.extract(X_attack, y_attack, thieved_classifier=clasificador_atacante)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10d473",
   "metadata": {},
   "source": [
    "### Evaluación de resultados - Ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9204ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo robado: 0.7812\n"
     ]
    }
   ],
   "source": [
    "preds = stolen_classifier.predict(X_validation)\n",
    "accuracy = np.mean(np.argmax(preds, axis=1) == np.argmax(y_validation, axis=1))\n",
    "print(f\"Precisión del modelo robado: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afca644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo víctima: 0.7812\n",
      "Precisión del modelo robado: 0.7812\n"
     ]
    }
   ],
   "source": [
    "preds_victima = clasificador_victima.predict(X_validation)\n",
    "acc_victima = np.mean(np.argmax(preds_victima, axis=1) == np.argmax(y_validation, axis=1))\n",
    "print(f\"Precisión del modelo víctima: {acc_victima:.4f}\")\n",
    "\n",
    "preds_robado = stolen_classifier.predict(X_validation)\n",
    "acc_robado = np.mean(np.argmax(preds_robado, axis=1) == np.argmax(y_validation, axis=1))\n",
    "print(f\"Precisión del modelo robado: {acc_robado:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa33454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 1: Real=1, Víctima=1, Robado=1\n",
      "Ejemplo 2: Real=5, Víctima=21, Robado=21\n",
      "Ejemplo 3: Real=9, Víctima=9, Robado=9\n",
      "Ejemplo 4: Real=10, Víctima=10, Robado=10\n",
      "Ejemplo 5: Real=15, Víctima=15, Robado=15\n",
      "Ejemplo 6: Real=21, Víctima=21, Robado=21\n",
      "Ejemplo 7: Real=7, Víctima=7, Robado=7\n",
      "Ejemplo 8: Real=20, Víctima=20, Robado=20\n",
      "Ejemplo 9: Real=20, Víctima=20, Robado=20\n",
      "Ejemplo 10: Real=8, Víctima=9, Robado=9\n",
      "\n",
      "Coincidencias entre víctima y robado: 32/32 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "labels_victima = np.argmax(preds_victima, axis=1)\n",
    "labels_robado = np.argmax(preds_robado, axis=1)\n",
    "labels_true = np.argmax(y_validation, axis=1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Ejemplo {i + 1}: Real={labels_true[i]}, Víctima={labels_victima[i]}, Robado={labels_robado[i]}\")\n",
    "\n",
    "coinciden = np.sum(labels_victima == labels_robado)\n",
    "print(f\"\\nCoincidencias entre víctima y robado: {coinciden}/{len(labels_true)} ({coinciden/len(labels_true):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "464c64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.6322 - accuracy: 0.7812\n",
      "1/1 [==============================] - 1s 1s/step - loss: 108.0209 - accuracy: 0.7812\n",
      "Original test loss: 0.63 vs stolen test loss: 108.02\n",
      "Original test accuracy: 0.78 vs stolen test accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Testing the performance of the original classifier\n",
    "score_original = vulnerable_model.evaluate(\n",
    "    x=X_validation,\n",
    "    y=y_validation\n",
    "    )\n",
    "\n",
    "# Testing the performance of the stolen classifier\n",
    "score_stolen = stolen_classifier.model.evaluate(\n",
    "    x=X_validation, \n",
    "    y=y_validation\n",
    "    )\n",
    "\n",
    "# Comparing test losses\n",
    "print(f\"Original test loss: {score_original[0]:.2f} \" \n",
    "      f\"vs stolen test loss: {score_stolen[0]:.2f}\")\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Original test accuracy: {score_original[1]:.2f} \" \n",
    "      f\"vs stolen test accuracy: {score_stolen[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f7aed",
   "metadata": {},
   "source": [
    "Como se pudo observar, el ataque logró replicar muy bien el comportamiento del modelo original (vulnerable_model). Haciendo que el modelo creado a partir de él, tuviera resultados parecidos. Aunque en cuanto a la predicción de los modelos se puede ver la misma, la pérdida es muy distinta. Mientras que la pérdida del modelo original es de tan solo 0.63, la del modelo robado es de 108.02, esto se puedo haber generado por cómo se entrenó este modelo robado. Haciendo que las distribuciones de probabilidad sean más dispersas que las del modelo original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae7b1f",
   "metadata": {},
   "source": [
    "## Defensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875fb6c1",
   "metadata": {},
   "source": [
    "### Implementación de la defensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b61c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_sigmoid = ReverseSigmoid(beta=3.5, apply_fit=False, apply_predict=True)\n",
    "clasificador_victima.postprocessing_defences = [reverse_sigmoid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f5f04",
   "metadata": {},
   "source": [
    "### Clasificador atacante (ya con modelo protegido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b295c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_model_protected = tf.keras.models.clone_model(vulnerable_model)\n",
    "attacker_model_protected.build(input_shape=(None, *input_shape))\n",
    "attacker_model_protected.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=loss_object,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "clasificador_atacante_protegido = TensorFlowV2Classifier(\n",
    "    model=attacker_model_protected,\n",
    "    loss_object=loss_object,\n",
    "    nb_classes=nb_classes,\n",
    "    input_shape=input_shape,\n",
    "    clip_values=(0, 1),\n",
    "    optimizer=attacker_model_protected.optimizer,\n",
    ")\n",
    "\n",
    "attack_protected = CopycatCNN(\n",
    "    batch_size_fit=32,\n",
    "    nb_epochs=20,\n",
    "    nb_stolen=len(X_attack),\n",
    "    classifier=clasificador_victima\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcad8df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ealva\\Documents\\GitHub\\SDC-Labs\\envLabs8_9\\lib\\site-packages\\art\\defences\\postprocessor\\reverse_sigmoid.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  perturbation_r = self.beta * (sigmoid(-self.gamma * np.log((1.0 - preds_clipped) / preds_clipped)) - 0.5)\n"
     ]
    }
   ],
   "source": [
    "stolen_classifier_protected = attack_protected.extract(X_attack, y_attack, thieved_classifier=clasificador_atacante_protegido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dabab272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 979ms/step - loss: 0.6322 - accuracy: 0.7812\n",
      "1/1 [==============================] - 1s 997ms/step - loss: 3859.0566 - accuracy: 0.0000e+00\n",
      "Original test loss: 0.63 vs stolen test loss: 3859.06\n",
      "Original test accuracy: 0.78 vs stolen test accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Testing the performance of the original classifier\n",
    "score_original = clasificador_victima.model.evaluate(\n",
    "    x=X_validation,\n",
    "    y=y_validation\n",
    "    )\n",
    "\n",
    "# Testing the performance of the stolen classifier\n",
    "score_stolen = stolen_classifier_protected.model.evaluate(\n",
    "    x=X_validation, \n",
    "    y=y_validation\n",
    "    )\n",
    "\n",
    "# Comparing test losses\n",
    "print(f\"Original test loss: {score_original[0]:.2f} \" \n",
    "      f\"vs stolen test loss: {score_stolen[0]:.2f}\")\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Original test accuracy: {score_original[1]:.2f} \" \n",
    "      f\"vs stolen test accuracy: {score_stolen[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e519ff",
   "metadata": {},
   "source": [
    "Listo, ahora se puede apreciar como el modelo robado no logró captar el comportamiento del modelo original. Haciendo que su precisión se redujera a 0, lo cuál parece excelente ya que nos asegura que un ataque de extracción haría que el modelo resultante tuviera una precisión muy baja. Como bien se había apreciado antes, aun sin la defensa, el modelo resultante del ataque ya contaba con una pérdida alta a comparación del original. Seguramente eso influyó en no tener la necesidad de implementar una defensa agresiva para hacer que el modelo atacante pudiera converger como lo hizo el modelo original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9aa77",
   "metadata": {},
   "source": [
    "# Segundo ataque - Ataque de Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdcb2e",
   "metadata": {},
   "source": [
    "## Ataque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b02d95",
   "metadata": {},
   "source": [
    "Como segundo ataque seleccioné lo que es un ataque de inferencia. Donde buscaré determianr si una muestra específica fue utilizada como parte del dataset para el entrenamiento del modelo. Entonces como resultado espero tener un modelo el cuál sea capaz de predecir si una imagen fue o no parte del set de entrenamiento del modelo original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d84ac",
   "metadata": {},
   "source": [
    "### Cargar el modelo víctima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "583a6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_model = load_model('../Laboratorio8/malimg_model_saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07643b4",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5edb0c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6094 images belonging to 22 classes.\n",
      "Found 1513 images belonging to 22 classes.\n",
      "Datos: X:(2560, 457, 340, 1) y:(2560, 22)\n"
     ]
    }
   ],
   "source": [
    "datasetPath = \"../Laboratorio8/malimg_paper_dataset_imgs\"\n",
    "avgHigh, avgWidth = 457, 340\n",
    "batch_size = 64\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "attack_generator = datagen.flow_from_directory(\n",
    "    datasetPath,\n",
    "    target_size=(avgHigh, avgWidth),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    datasetPath,\n",
    "    target_size=(avgHigh, avgWidth),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for _ in range(40):  # 40 batches de 64 = 2560 muestras\n",
    "    Xb, yb = next(attack_generator)\n",
    "    X_list.append(Xb)\n",
    "    y_list.append(yb)\n",
    "X_attack = np.concatenate(X_list)\n",
    "y_attack = np.concatenate(y_list)\n",
    "X_validation, y_validation = next(validation_generator)\n",
    "print(f\"Datos: X:{X_attack.shape} y:{y_attack.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec026dc",
   "metadata": {},
   "source": [
    "### Clasificador víctima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "949cf297",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "nb_classes = vulnerable_model.output_shape[-1]\n",
    "input_shape = vulnerable_model.input_shape[1:]\n",
    "\n",
    "clasificador_victima = TensorFlowV2Classifier(\n",
    "    model=vulnerable_model,\n",
    "    loss_object=loss_object,\n",
    "    nb_classes=nb_classes,\n",
    "    input_shape=input_shape,\n",
    "    clip_values=(0, 1),\n",
    "    optimizer=vulnerable_model.optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6216c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_members = X_attack[:1000]\n",
    "y_members = y_attack[:1000]\n",
    "X_non_members = X_validation[:1000]\n",
    "y_non_members = y_validation[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2c5fe3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_attack = MembershipInferenceBlackBoxRuleBased(classifier=clasificador_victima)\n",
    "mia_data = np.concatenate([X_members, X_non_members])\n",
    "mia_labels = np.concatenate([np.ones(len(X_members)), np.zeros(len(X_non_members))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "acb4dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clasificador_victima.predict(mia_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "86445643",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = mia_attack.infer(x=mia_data, y=mia_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e57b8",
   "metadata": {},
   "source": [
    "### Evaluación de resultados - Ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f38f8d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del ataque de inferencia: 0.0465\n"
     ]
    }
   ],
   "source": [
    "accuracy_inference = np.mean(pred_labels == mia_labels)\n",
    "print(f\"Precisión del ataque de inferencia: {accuracy_inference:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb0642",
   "metadata": {},
   "source": [
    "## Defensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5ebd8",
   "metadata": {},
   "source": [
    "### Implementación de la defensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc34ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgm = FastGradientMethod(estimator=clasificador_victima, eps=0.03)\n",
    "\n",
    "adv_trainer = AdversarialTrainer(classifier=clasificador_victima, attacks=fgm, ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4186564",
   "metadata": {},
   "source": [
    "### Clasificador atacante (ya con modelo protegido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "39b8990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precompute adv samples: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Adversarial training epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 204 calls to <function TensorFlowV2Classifier.fit.<locals>.train_step at 0x0000020146966290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 205 calls to <function TensorFlowV2Classifier.fit.<locals>.train_step at 0x0000020146966290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adversarial training epochs: 100%|██████████| 5/5 [1:03:32<00:00, 762.42s/it]\n"
     ]
    }
   ],
   "source": [
    "adv_trainer.fit(X_attack, y_attack, nb_epochs=5, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5694eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del ataque de inferencia (después de la defensa): 0.0465\n"
     ]
    }
   ],
   "source": [
    "clasificador_victima_defendido = TensorFlowV2Classifier(\n",
    "    model=clasificador_victima.model,\n",
    "    loss_object=loss_object,\n",
    "    nb_classes=nb_classes,\n",
    "    input_shape=input_shape,\n",
    "    clip_values=(0, 1)\n",
    ")\n",
    "\n",
    "mia_attack_defended = MembershipInferenceBlackBoxRuleBased(classifier=clasificador_victima_defendido)\n",
    "\n",
    "pred_labels_defended = mia_attack_defended.infer(x=mia_data, y=mia_labels)\n",
    "\n",
    "accuracy_inference_defended = np.mean(pred_labels_defended == mia_labels)\n",
    "print(f\"Precisión del ataque de inferencia (después de la defensa): {accuracy_inference_defended:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLabs8_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
